{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6885d37d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e960f79f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>npreg</th>\n",
       "      <th>glu</th>\n",
       "      <th>bp</th>\n",
       "      <th>skin</th>\n",
       "      <th>bmi</th>\n",
       "      <th>ped</th>\n",
       "      <th>age</th>\n",
       "      <th>type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5</td>\n",
       "      <td>86</td>\n",
       "      <td>68</td>\n",
       "      <td>28</td>\n",
       "      <td>30.2</td>\n",
       "      <td>0.364</td>\n",
       "      <td>24</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>7</td>\n",
       "      <td>195</td>\n",
       "      <td>70</td>\n",
       "      <td>33</td>\n",
       "      <td>25.1</td>\n",
       "      <td>0.163</td>\n",
       "      <td>55</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>77</td>\n",
       "      <td>82</td>\n",
       "      <td>41</td>\n",
       "      <td>35.8</td>\n",
       "      <td>0.156</td>\n",
       "      <td>35</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>165</td>\n",
       "      <td>76</td>\n",
       "      <td>43</td>\n",
       "      <td>47.9</td>\n",
       "      <td>0.259</td>\n",
       "      <td>26</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>107</td>\n",
       "      <td>60</td>\n",
       "      <td>25</td>\n",
       "      <td>26.4</td>\n",
       "      <td>0.133</td>\n",
       "      <td>23</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   npreg  glu  bp  skin   bmi    ped  age  type\n",
       "0      5   86  68    28  30.2  0.364   24     0\n",
       "1      7  195  70    33  25.1  0.163   55     1\n",
       "2      5   77  82    41  35.8  0.156   35     0\n",
       "3      0  165  76    43  47.9  0.259   26     0\n",
       "4      0  107  60    25  26.4  0.133   23     0"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "6c0b6fc1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exercise 1.a\n",
      "------------\n",
      "TP: 66\n",
      "FP: 24\n",
      "TN: 199\n",
      "FN: 43\n",
      "Accuracy: 0.798\n",
      "------------\n",
      "Exercise 1.b\n",
      "Although the accuracy of the LDA model is a bit less (0.771) in comparison with the LogisticRegression model and it might look like LogisticRegression is better, still, I would choose the LDA model, since for any model accuracy is not the only one parameter that sould be considered, recall and precision are also important, and for disease, detection models are really important TP, FP, TN, FN values, especially FN is a number of people which was predicted wrongly and the model says the negative answer, people are really whereas, in reality, they are not. And if we compare this value for both methods we see that LDA has less value than the LogisticRegression model, so LDA is better in that perspective.\n",
      "------------\n",
      "Exercise 1.c\n",
      "Different sources say that LDA is better than LogisticRegression when all requirements are met. So, LDA requires that all features are continuous (not categorical) and follow a Normal distribution. That makes LDA sensible to outliers and not applicable for categorical data, so in an unknown unpurified dataset, I would choose LogisticRegression as a more robust method.\n",
      "------------\n",
      "Exercise 1.d\n",
      "The coefficient for npreg is 0.3348. Calculating the exponential function results in 1.3977, which amounts to an increase in diabetes risk of 39.7654 percent per additional pregnancy.\n",
      "------------\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Skeleton for Homework 4: Logistic Regression and Decision Trees\n",
    "Part 1: Logistic Regression\n",
    "\n",
    "Authors: Anja Gumpinger, Dean Bodenham, Bastian Rieck\n",
    "'''\n",
    "\n",
    "#!/usr/bin/env python3\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "\n",
    "def compute_metrics(y_true, y_pred):\n",
    "    '''\n",
    "    Computes several quality metrics of the predicted labels and prints\n",
    "    them to `stdout`.\n",
    "\n",
    "    :param y_true: true class labels\n",
    "    :param y_pred: predicted class labels\n",
    "    '''\n",
    "\n",
    "    tn, fp, fn, tp = confusion_matrix(y_true, y_pred).ravel()\n",
    "\n",
    "    print('Exercise 1.a')\n",
    "    print('------------')\n",
    "    print('TP: {0:d}'.format(tp))\n",
    "    print('FP: {0:d}'.format(fp))\n",
    "    print('TN: {0:d}'.format(tn))\n",
    "    print('FN: {0:d}'.format(fn))\n",
    "    print('Accuracy: {0:.3f}'.format(accuracy_score(y_true, y_pred)))\n",
    "\n",
    "def load_data(path):\n",
    "    #read data from file using pandas\n",
    "    df = pd.read_csv(path)\n",
    "    # extract first 7 columns to data matrix X (actually, a numpy ndarray)\n",
    "    X = df.iloc[:, 0:7].values\n",
    "\n",
    "    # extract 8th column (labels) to numpy array\n",
    "    y = df.iloc[:, 7].values\n",
    "    return X, y\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "###################################################################\n",
    "# Your code goes here.\n",
    "###################################################################\n",
    "\n",
    "    path = 'data/diabetes_train.csv'\n",
    "    X_train, y_train = load_data(path)\n",
    "\n",
    "    scaler = StandardScaler()\n",
    "    scaler.fit(X_train)\n",
    "    X_train_scaled = scaler.transform(X_train)\n",
    "\n",
    "    model = LogisticRegression(C=1)\n",
    "    model.fit(X_train_scaled, y_train)\n",
    "\n",
    "    path = 'data/diabetes_test.csv'\n",
    "    X_test, y_test = load_data(path)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "    y_pred = model.predict(X_test_scaled)\n",
    "\n",
    "    compute_metrics(y_test, y_pred)\n",
    "    print('------------')\n",
    "\n",
    "    print('Exercise 1.b')\n",
    "    print('Although the accuracy of the LDA model is a bit less (0.771) in comparison with the LogisticRegression model and it might look like LogisticRegression is better, still, I would choose the LDA model, since for any model accuracy is not the only one parameter that sould be considered, recall and precision are also important, and for disease, detection models are really important TP, FP, TN, FN values, especially FN is a number of people which was predicted wrongly and the model says the negative answer, people are really whereas, in reality, they are not. And if we compare this value for both methods we see that LDA has less value than the LogisticRegression model, so LDA is better in that perspective.')\n",
    "    print('------------')\n",
    "\n",
    "    print('Exercise 1.c')\n",
    "    print('Different sources say that LDA is better than LogisticRegression when all requirements are met. So, LDA requires that all features are continuous (not categorical) and follow a Normal distribution. That makes LDA sensible to outliers and not applicable for categorical data, so in an unknown unpurified dataset, I would choose LogisticRegression as a more robust method.')\n",
    "    print('------------')\n",
    "\n",
    "    print('Exercise 1.d')\n",
    "    npre_c = model.coef_[0][0]\n",
    "    prob = np.exp(npre_c)\n",
    "    print(\"The coefficient for npreg is {}. Calculating the exponential function results in {}, which amounts to an increase in diabetes risk of {} percent per additional pregnancy.\"\n",
    "          .format(round(npre_c, 4), round(prob, 4), round((prob - 1) * 100, 4)))\n",
    "    print('------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "72dc5ffc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9182958340544894\n",
      "------------\n",
      "Exercise 2.b\n",
      "IG(sepal length < 5.5): 0.5511234895788772\n",
      "IG(sepal width < 3.0): 0.2486661749461121\n",
      "IG(petal length < 2.0): 0.9182958340544894\n",
      "IG(petal width < 1.0): 0.9182958340544894\n",
      "------------\n",
      "Exercise 2.c\n",
      "I would select (petal length < 2.0) or (petal width < 1.0) to be the first split, because they have the biggest information gain\n",
      "------------\n",
      "Exercise 2.d\n",
      "Original dataset\n",
      "Feature importances:  [0.01667014 0.01667014 0.38926487 0.57739485]\n",
      "Feature importances:  [0.01072211 0.03692821 0.06763852 0.88471116]\n",
      "Feature importances:  [0.00750626 0.         0.55834592 0.43414782]\n",
      "Feature importances:  [0.01251043 0.01251043 0.05712101 0.91785814]\n",
      "Feature importances:  [0.01669101 0.         0.04721995 0.93608905]\n",
      "Reduced dataset\n",
      "Feature importances:  [0. 0. 1. 0.]\n",
      "Feature importances:  [0. 0. 1. 0.]\n",
      "Feature importances:  [0. 0. 1. 0.]\n",
      "Feature importances:  [0. 0. 1. 0.]\n",
      "Feature importances:  [0. 0. 1. 0.]\n",
      "Accuracy score using cross-validation\n",
      "The mean accuracy is 0.95\n",
      "-------------------------------------\n",
      "\n",
      "Feature importances for _original_ data set\n",
      "For the original data, the two most important features are:\n",
      "- petal length; \n",
      "- petal width;\n",
      "-------------------------------------------\n",
      "\n",
      "Feature importances for _reduced_ data set\n",
      "For the reduced data, the most important feature is petal length. This means that petal width feature is distinuasible one for class 2\n",
      "------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "\n",
    "'''\n",
    "Skeleton for Homework 4: Logistic Regression and Decision Trees\n",
    "Part 2: Decision Trees\n",
    "\n",
    "Authors: Anja Gumpinger, Bastian Rieck\n",
    "'''\n",
    "\n",
    "import numpy as np\n",
    "import sklearn.datasets\n",
    "from sklearn.datasets import load_iris\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "\n",
    "    iris = sklearn.datasets.load_iris()\n",
    "    X = iris.data\n",
    "    y = iris.target\n",
    "\n",
    "    feature_names = iris.feature_names\n",
    "    num_features = len(set(feature_names))\n",
    "\n",
    "    ####################################################################\n",
    "    # Your code goes here.\n",
    "    ####################################################################\n",
    "    def compute_entropy(probs):\n",
    "        return -np.sum(np.log2(probs) * probs)\n",
    "\n",
    "    def split_data(X, y, attribute_index, theta):\n",
    "        D = X[:,attribute_index]\n",
    "        y1 = y[D < theta]\n",
    "        y2 = y[D >= theta]\n",
    "        return y1, y2\n",
    "\n",
    "    def compute_information_content(y):\n",
    "        unique, counts = np.unique(y, return_counts=True)\n",
    "        return compute_entropy(counts / len(y))\n",
    "\n",
    "    def compute_information_a(X, y, attribute_index, theta):\n",
    "        y1, y2 = split_data(X, y, attribute_index, theta)\n",
    "        ic1 = compute_information_content(y1)\n",
    "        ic2 = compute_information_content(y2)\n",
    "        return (ic1 * len(y1) + ic2 * len(y2)) / (len(y1) + len(y2))\n",
    "\n",
    "    def compute_information_gain(X, y, attribute_index, theta):\n",
    "        return compute_information_content(y) - compute_information_a(X, y, attribute_index, theta)\n",
    "\n",
    "    attribute_index = 3\n",
    "    theta = 1.0\n",
    "    information_gain = compute_information_gain(X, y, attribute_index, theta)\n",
    "    print(information_gain)\n",
    "    print('------------')\n",
    "\n",
    "    print('Exercise 2.b')\n",
    "\n",
    "    #1. sepal length < 5.5\n",
    "    print(\"IG(sepal length < 5.5):\", compute_information_gain(X, y, 0, 5.5))\n",
    "    #2. sepal width < 3.0\n",
    "    print(\"IG(sepal width < 3.0):\", compute_information_gain(X, y, 1, 3.0))\n",
    "    #3. petal length < 2.0\n",
    "    print(\"IG(petal length < 2.0):\", compute_information_gain(X, y, 2, 2.0))\n",
    "    #4. petal width < 1.0\n",
    "    print(\"IG(petal width < 1.0):\", compute_information_gain(X, y, 3, 1.0))\n",
    "\n",
    "    print('------------')\n",
    "\n",
    "    print('Exercise 2.c')\n",
    "    print('I would select (petal length < 2.0) or (petal width < 1.0) to be the first split, because they have the biggest information gain')\n",
    "    print('------------')\n",
    "\n",
    "    ####################################################################\n",
    "    # Exercise 2.d\n",
    "    ####################################################################\n",
    "\n",
    "    # Do _not_ remove this line because you will get different splits\n",
    "    # which make your results different from the expected ones...\n",
    "    np.random.seed(42)\n",
    "    print('Exercise 2.d')\n",
    "\n",
    "    from sklearn.model_selection import KFold\n",
    "    from sklearn import tree\n",
    "    from sklearn.metrics import accuracy_score\n",
    "\n",
    "    def process(X, y):\n",
    "        accs = []\n",
    "        cv = KFold(n_splits=5, shuffle=True)\n",
    "        for train_index, test_index in cv.split(X, y):\n",
    "            X_train, y_train = X[train_index], y[train_index]\n",
    "            X_test, y_test = X[test_index], y[test_index]\n",
    "            model = tree.DecisionTreeClassifier()\n",
    "            model.fit(X_train, y_train)\n",
    "            y_pred = model.predict(X_test)\n",
    "            accs.append(accuracy_score(y_test, y_pred))\n",
    "            print(\"Feature importances: \", model.feature_importances_)\n",
    "        accs = np.array(accs)\n",
    "        return accs\n",
    "    print('Original dataset')\n",
    "    accs = process(X, y)\n",
    "    print('Reduced dataset')\n",
    "    X = X[y != 2]\n",
    "    y = y[y != 2]\n",
    "    accs2 = process(X, y)\n",
    "\n",
    "    print('Accuracy score using cross-validation')\n",
    "    print('The mean accuracy is', np.round(accs.mean(), 2))\n",
    "    print('-------------------------------------\\n')\n",
    "\n",
    "    print('Feature importances for _original_ data set')\n",
    "    print('For the original data, the two most important features are:\\n- petal length; \\n- petal width;')\n",
    "    print('-------------------------------------------\\n')\n",
    "\n",
    "    print('Feature importances for _reduced_ data set')\n",
    "    print('For the reduced data, the most important feature is petal length. This means that petal width feature is distinuasible one for class 2')\n",
    "    print('------------------------------------------\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b8df310",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
